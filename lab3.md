---
title: "Lab Session 3: Auditory Perception"
layout: default
---

# Lab Session 3: Auditory Perception

As part of this lab you will act as a test subject in some simple
auditory psychophysics experiments, which run in your web browser.
You will need to use headphones. Please run the **calibration** test
first and adjust your headphone volume to a safe and comfortable level.

**Privacy note**: You will be given the opportunity to download your results in CSV format.
Your data is not otherwise stored or used in any way except for local presentation to you.


## Experiments

As in week 3, not all of these are strictly *experiments* — some are probably
better thought of as demonstrations.

* [Calibration](experiments/calibration/?home=/lab3.html)
* [Stereo localisation](experiments/stereo/?home=/lab3.html)
* [Pitch sensitivity](experiments/pitch/?home=/lab3.html)
* [Binaural beats](experiments/binaural/?home=/lab3.html)
* [Haas effect](experiments/haas/?home=/lab3.html)


## Demos

In between experiments, we'll probably have a look* at
some famous auditory illusions and effects.

* Shepard tones & scales
* Risset's accelerando
* Deutsch scale illusion
* McGurk effect
* Franssen effect

<small>* To use an inappropriate verb. Recall the ubiquity of
visual language discussed last week!</small>


## Discussion Points

* How is audio typically used in software interfaces?
* Can you distinguish different *classes* of such audio use?
* As noted in the experiments, web audio usually requires explicit
  user authorisation. Why?
* How does audio differ from vision in this respect?


## Meta Discussion Points

Based on the pulse survey feedback, we might also talk about
possible activities for future practicals.


## Extras: Generating Music with Deep Learning

If you are extremely dedicated and/or bored, you could also have a look at the
following tutorial sessions on music generation. These are primarily aimed
at students on the COMP0161 Auditory Computing module, but you might find them
interesting as well. They are implemented as Colab notebooks — click the
"open in Colab" badge to open the corresponding notebook.

(Note: you may receive a warning about the notebooks not being from Google, which
is of course true — they're from me. Only you can decide whether or not to accept
that risk.)

The three sessions build on one another, but data is provided so you can run a
later one without having done its predecessors if you wish.

* **Lab 1: Data** — build a text-encoded dataset of classical piano music
    * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab1.ipynb)
* **Lab 2: Learning** — train a small GPT-style model to generate music in a similar style
    * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab2.ipynb)
* **Lab 3: Synthesis & Effects** — tweak the instrument sound and apply a variety of audio effects
    * [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab3.ipynb)
